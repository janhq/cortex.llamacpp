From 0d61470625e9e4a9c1a9e88164c85773d32f6ee2 Mon Sep 17 00:00:00 2001
From: James Nguyen <jamesnguyen@Jamess-Laptop.local>
Date: Mon, 30 Sep 2024 15:51:16 +0700
Subject: [PATCH] Add API query buffer size

diff --git a/include/llama.h b/include/llama.h
index 25a9f827..7ac85597 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -471,6 +471,8 @@ extern "C" {
     DEPRECATED(LLAMA_API int32_t llama_n_vocab    (const struct llama_vocab * vocab), "use llama_vocab_n_tokens instead");
 
     LLAMA_API const struct llama_model * llama_get_model   (const struct llama_context * ctx);
+    LLAMA_API size_t llama_get_cpu_buffer(const struct llama_model * model);
+    LLAMA_API size_t llama_get_other_buffer(const struct llama_model * model);
     LLAMA_API    struct llama_kv_cache * llama_get_kv_self (      struct llama_context * ctx);
     LLAMA_API  enum llama_pooling_type   llama_pooling_type(const struct llama_context * ctx); // TODO: rename to llama_get_pooling_type
 
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index aa363df6..a21aba4a 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -2367,6 +2367,14 @@ const llama_model * llama_get_model(const llama_context * ctx) {
     return &ctx->get_model();
 }
 
+size_t llama_get_cpu_buffer(const struct llama_model * model) {
+    return model->llama_get_cpu_buffer();
+}
+
+size_t llama_get_other_buffer(const struct llama_model * model) {
+    return model->llama_get_other_buffer();
+}
+
 llama_kv_cache * llama_get_kv_self(llama_context * ctx) {
     return ctx->get_kv_self();
 }
diff --git a/src/llama-model.cpp b/src/llama-model.cpp
index 0ae75415..94799efb 100644
--- a/src/llama-model.cpp
+++ b/src/llama-model.cpp
@@ -4072,6 +4072,26 @@ const ggml_tensor * llama_model::get_tensor(const char * name) const {
     return it->second;
 }
 
+size_t llama_model::llama_get_cpu_buffer() const {
+    size_t buffer{0};
+    for (const auto& buf : pimpl->bufs) {
+        if (strcmp(ggml_backend_buffer_name(buf.get()), "CPU") == 0) {
+            buffer += ggml_backend_buffer_get_size(buf.get());
+        }
+    }
+    return buffer;
+}
+
+size_t llama_model::llama_get_other_buffer() const {
+    size_t buffer{0};
+    for (const auto& buf : pimpl->bufs) {
+        if (strcmp(ggml_backend_buffer_name(buf.get()), "CPU") != 0) {
+            buffer += ggml_backend_buffer_get_size(buf.get());
+        }
+    }
+    return buffer;
+}
+
 struct llm_build_llama : public llm_graph_context {
     llm_build_llama(const llama_model & model, const llm_graph_params & params, ggml_cgraph * gf) : llm_graph_context(params) {
         const int64_t n_embd_head = hparams.n_embd_head_v;
diff --git a/src/llama-model.h b/src/llama-model.h
index a9da1215..1790f227 100644
--- a/src/llama-model.h
+++ b/src/llama-model.h
@@ -382,6 +382,10 @@ struct llama_model {
 
     const struct ggml_tensor * get_tensor(const char * name) const;
 
+    size_t llama_get_cpu_buffer() const;
+
+    size_t llama_get_other_buffer() const;
+
     // TODO: move this to new llm_arch_model_i interface
     llama_memory_i * create_memory() const; // TODO: params
 

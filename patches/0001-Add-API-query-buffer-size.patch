From 0d61470625e9e4a9c1a9e88164c85773d32f6ee2 Mon Sep 17 00:00:00 2001
From: James Nguyen <jamesnguyen@Jamess-Laptop.local>
Date: Mon, 30 Sep 2024 15:51:16 +0700
Subject: [PATCH] Add API query buffer size

---
 include/llama.h |  3 +++
 src/llama.cpp   | 20 ++++++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/include/llama.h b/include/llama.h
index 7cae1bbe..fdcbf949 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -447,6 +447,9 @@ extern "C" {
 
     LLAMA_API const struct llama_model * llama_get_model(const struct llama_context * ctx);
 
+    LLAMA_API size_t const llama_get_cpu_buffer(const struct llama_model * model);
+    LLAMA_API size_t const llama_get_other_buffer(const struct llama_model * model);
+
     LLAMA_API enum llama_pooling_type llama_pooling_type(const struct llama_context * ctx);
     LLAMA_API enum llama_vocab_type   llama_vocab_type  (const struct llama_model * model);
     LLAMA_API enum llama_rope_type    llama_rope_type   (const struct llama_model * model);
diff --git a/src/llama.cpp b/src/llama.cpp
index c466cd88..15f3102c 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -19561,6 +19561,26 @@ const struct llama_model * llama_get_model(const struct llama_context * ctx) {
     return &ctx->model;
 }
 
+const size_t llama_get_cpu_buffer(const struct llama_model * model) {
+    size_t buffer{0};
+    for (const auto buf : model->bufs) {
+        if (strcmp(ggml_backend_buffer_name(buf), "CPU") == 0) {
+            buffer += ggml_backend_buffer_get_size(buf);
+        }
+    }
+    return buffer;
+}
+
+const size_t llama_get_other_buffer(const struct llama_model * model) {
+    size_t buffer{0};
+    for (const auto buf : model->bufs) {
+        if (strcmp(ggml_backend_buffer_name(buf), "CPU") != 0) {
+            buffer += ggml_backend_buffer_get_size(buf);
+        }
+    }
+    return buffer;
+}
+
 enum llama_pooling_type llama_pooling_type(const struct llama_context * ctx) {
     return ctx->cparams.pooling_type;
 }
-- 
2.39.5 (Apple Git-154)


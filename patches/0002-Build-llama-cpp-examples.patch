diff --git a/examples/CMakeLists.txt b/examples/CMakeLists.txt
index 21b31392..18049f0b 100644
--- a/examples/CMakeLists.txt
+++ b/examples/CMakeLists.txt
@@ -61,7 +61,7 @@ else()
             # disabled on Windows because it uses internal functions not exported with LLAMA_API
             add_subdirectory(quantize-stats)
         endif()
-        add_subdirectory(llava)
+        # add_subdirectory(llava)
         if (GGML_RPC)
             add_subdirectory(rpc)
         endif()
diff --git a/examples/server/CMakeLists.txt b/examples/server/CMakeLists.txt
index 1b7cc8c1..269d8397 100644
--- a/examples/server/CMakeLists.txt
+++ b/examples/server/CMakeLists.txt
@@ -34,7 +34,7 @@ endforeach()
 add_executable(${TARGET} ${TARGET_SRCS})
 install(TARGETS ${TARGET} RUNTIME)
 
-target_include_directories(${TARGET} PRIVATE ${CMAKE_SOURCE_DIR})
+target_include_directories(${TARGET} PRIVATE ${CMAKE_SOURCE_DIR}/llama.cpp)
 target_link_libraries(${TARGET} PRIVATE common ${CMAKE_THREAD_LIBS_INIT})
 
 if (LLAMA_SERVER_SSL)
diff --git a/include/llama.h b/include/llama.h
index efbb27d2..1d0e0e41 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -448,6 +448,9 @@ extern "C" {
 
     LLAMA_API const struct llama_model * llama_get_model(const struct llama_context * ctx);
 
+    LLAMA_API size_t const llama_get_cpu_buffer(const struct llama_model * model);
+    LLAMA_API size_t const llama_get_other_buffer(const struct llama_model * model);
+
     LLAMA_API enum llama_pooling_type llama_pooling_type(const struct llama_context * ctx);
     LLAMA_API enum llama_vocab_type   llama_vocab_type  (const struct llama_model * model);
     LLAMA_API enum llama_rope_type    llama_rope_type   (const struct llama_model * model);
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 38a55fb2..80b3532e 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -602,6 +602,26 @@ const struct llama_model * llama_get_model(const struct llama_context * ctx) {
     return &ctx->model;
 }
 
+const size_t llama_get_cpu_buffer(const struct llama_model * model) {
+    size_t buffer{0};
+    for (const auto& buf : model->bufs) {
+        if (strcmp(ggml_backend_buffer_name(buf.get()), "CPU") == 0) {
+            buffer += ggml_backend_buffer_get_size(buf.get());
+        }
+    }
+    return buffer;
+}
+
+const size_t llama_get_other_buffer(const struct llama_model * model) {
+    size_t buffer{0};
+    for (const auto& buf : model->bufs) {
+        if (strcmp(ggml_backend_buffer_name(buf.get()), "CPU") != 0) {
+            buffer += ggml_backend_buffer_get_size(buf.get());
+        }
+    }
+    return buffer;
+}
+
 enum llama_pooling_type llama_pooling_type(const struct llama_context * ctx) {
     return ctx->cparams.pooling_type;
 }

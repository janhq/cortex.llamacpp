name: Convert model to gguf all quants

on:
  push:
    branches:
      - feat-all-quants-ci
  # workflow_dispatch:
  #   inputs:
  #     source_model_id:
  #       description: "Source HuggingFace model ID to pull. For ex: meta-llama/Meta-Llama-3.1-8B-Instruct"
  #       required: true
  #     source_model_size:
  #       description: "The model size. For ex: 8b"
  #       required: true
  #       type: string
  #     target_model_id:
  #       description: "Target HuggingFace model ID to push. For ex: llama3.1"
  #       required: true
  #       type: string

  
env:
  USER_NAME: cortexso
  SOURCE_MODEL_ID:  meta-llama/Meta-Llama-3-8B-Instruct #${{ inputs.source_model_id }}
  SOURCE_MODEL_SIZE: 8b #${{ inputs.source_model_size }} 
  TARGET_MODEL_ID: llama3 #${{ inputs.target_model_id }} 

jobs:
  converter:
    runs-on: ubuntu-20-04-gguf
    steps:
      - name: Checkout
        uses: actions/checkout@v4 # v4.1.7
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5 # v5.1.1
        with:
          python-version: '3.12'
          # architecture: 'x64'

      - name: Cache Python packages
        uses: actions/cache@0c45773b623bea8c8e75f6c82b208c3cf94ea4f9 # v4.0.2
        with:
          path: |
            ~/.cache/pip
            ~/.local/share/pip
            .venv
          key: ${{ runner.os }}-pip-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip3 install -r llama.cpp/requirements.txt
          pip3 install hf-transfer
          git lfs install

      - name: Extract MODEL_NAME
        run: |
          SOURCE_MODEL_ID="${{ env.SOURCE_MODEL_ID }}"
          MODEL_NAME="$(echo $SOURCE_MODEL_ID | rev | cut -d/ -f1 | rev)"
          echo $MODEL_NAME
          MODEL_NAME="$(echo $MODEL_NAME | tr '[:upper:]' '[:lower:]')"
          echo $MODEL_NAME
          echo "MODEL_NAME=$MODEL_NAME" >> $GITHUB_ENV

      - name: Print environment variables
        run: |
          echo "SOURCE_MODEL_ID: ${{ env.SOURCE_MODEL_ID }}"
          echo "MODEL_NAME: ${{ env.MODEL_NAME }}"
      - name: Check file existence
        id: check_files
        uses: andstor/file-existence-action@v1
        with:
          files: "/mnt/models/${{ env.MODEL_NAME }}/hf"


      - name: Prepare folders
        if: steps.check_files.outputs.files_exists == 'false'
        run: |
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/hf
          
      
      - name: Download Hugging Face model
        id: download_hf
        if: steps.check_files.outputs.files_exists == 'false'
        run: |
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN_READ }} --add-to-git-credential
          huggingface-cli download --repo-type model --local-dir /mnt/models/${{ env.MODEL_NAME }}/hf ${{ env.SOURCE_MODEL_ID }}
          huggingface-cli logout

      - name: Build lib for quantize
        run: |
          cd llama.cpp && make
          cd ../../

      - name: Convert to GGUF
        run: |
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN_READ }} --add-to-git-credential
          python3 llama.cpp/convert_hf_to_gguf.py "/mnt/models/${{ env.MODEL_NAME }}/hf" --outfile "/mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf"
          huggingface-cli logout
      
      - name: Quantize and Upload 
        run: |
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q2-k/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q2-k/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q2-k/model.gguf Q2_K
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-ks/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-ks/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-ks/model.gguf Q3_K_S
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-km/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-km/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-km/model.gguf Q3_K_M
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-kl/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-kl/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q3-kl/model.gguf Q3_K_L
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-ks/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-ks/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-ks/model.gguf Q4_K_S
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/model.gguf Q4_K_M
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q5-ks/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q5-ks/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q5-ks/model.gguf Q5_K_S
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q5-km/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q5-km/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q5-km/model.gguf Q5_K_M
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q6-k/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q6-k/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q6-k/model.gguf Q6_K
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q8-0/
          [ ! -f /mnt/models/${{ env.MODEL_NAME }}/gguf/q8-0/model.gguf ] &&  ./llama.cpp/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q8-0/model.gguf Q8_0
          rm -rf /mnt/models/${{ env.MODEL_NAME }}/gguf/model-origin.gguf

      - name: Upload to Hugging Face
        run: |
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN_WRITE }} --add-to-git-credential
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q2-k/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q2-k"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q3-ks/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q3-ks"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q3-km/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q3-km"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q3-kl/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q3-kl"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q4-ks/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q4-ks"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q4-km"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q5-ks/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q5-ks"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q5-km/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q5-km"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q6-k/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q6-k"
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q8-0/" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf-q8-0"
          rm -rf /mnt/models/${{ env.MODEL_NAME }}/gguf/*
          huggingface-cli logout
          rm -rf llama.cpp/build/


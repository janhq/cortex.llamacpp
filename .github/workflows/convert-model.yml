name: Convert model to gguf

on:
  push:
    branches:
      - feat-CI-for-convert-model
  workflow_dispatch:
    inputs:
      source_model_id:
        description: "Source HuggingFace model ID to pull. For ex: meta-llama/Meta-Llama-3.1-8B-Instruct"
        required: true
      source_model_size:
        description: "The model size. For ex: 8b"
        required: true
        type: string
      target_model_id:
        description: "Target HuggingFace model ID to push. For ex: llama3.1"
        required: true
        type: string

  
env:
  USER_NAME: cortexso
  SOURCE_MODEL_ID:  meta-llama/Meta-Llama-3-8B-Instruct #${{ inputs.source_model_id }}
  SOURCE_MODEL_SIZE: 8b #${{ inputs.source_model_size }}
  TARGET_MODEL_ID:  llama3 #${{ inputs.target_model_id }}
  PRECISION: Q4_K_M                   # Valid values: int4,fp16,fp3

jobs:
  converter:
    runs-on: ubuntu-20-04-gguf
    steps:
      - name: Checkout
        uses: actions/checkout@v4 # v4.1.7
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5 # v5.1.1
        with:
          python-version: '3.12'
          # architecture: 'x64'

      - name: Cache Python packages
        uses: actions/cache@0c45773b623bea8c8e75f6c82b208c3cf94ea4f9 # v4.0.2
        with:
          path: |
            ~/.cache/pip
            ~/.local/share/pip
            .venv
          key: ${{ runner.os }}-pip-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip3 install -r llama.cpp/requirements.txt

      - name: Extract MODEL_NAME
        id: vars
        run: |
          SOURCE_MODEL_ID="${{ env.SOURCE_MODEL_ID }}"
          MODEL_NAME="$(echo $SOURCE_MODEL_ID | rev | cut -d/ -f1 | rev)"
          MODEL_NAME="$(echo "$MODEL_NAME" | tr '[:upper:]' '[:lower:]')"
          echo "MODEL_NAME=$MODEL_NAME" >> $env:GITHUB_OUTPUT

      - name: Print environment variables
        run: |
          echo "SOURCE_MODEL_ID: ${{ env.SOURCE_MODEL_ID }}"
          echo "PRECISION: ${{ env.PRECISION }}"
          echo "MODEL_NAME: ${{ steps.vars.outputs.MODEL_NAME }}"
      - name: Check file existence
        id: check_files
        uses: andstor/file-existence-action@v1
        with:
          files: "/mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/hf"


      - name: Prepare folders
        if: steps.check_files.outputs.files_exists == 'false'
        run: |
          mkdir -p /mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/hf
          
      
      - name: Download Hugging Face model
        id: download_hf
        if: steps.check_files.outputs.files_exists == 'false'
        run: |
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN_READ }} --add-to-git-credential
          huggingface-cli download --repo-type model --local-dir /mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/hf ${{ env.SOURCE_MODEL_ID }}
          huggingface-cli logout

      - name: Build lib for quantize
        run: |
          cd llama.cpp && mkdir build && cd build
          cmake .. && cmake --build . --config Release -j 32
          cd ../../

      - name: Convert to GGUF
        run: |
          mkdir -p /mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/gguf
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN_READ }} --add-to-git-credential
          python3 llama.cpp/convert_hf_to_gguf.py "/mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/hf" --outfile "/mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/gguf/model-origin.gguf"
          huggingface-cli logout
      
      - name: Quantize model
        run: |
          ./llama.cpp/build/bin/llama-quantize /mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/gguf/model-origin.gguf /mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/gguf/model.gguf ${{ env.PRECISION }}
          rm -rf /mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/gguf/model-origin.gguf

      - name: Upload to Hugging Face
        run: |
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN_WRITE }} --add-to-git-credential
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ steps.vars.outputs.MODEL_NAME }}/gguf" . --revision "${{ env.SOURCE_MODEL_SIZE }}-gguf"
          huggingface-cli logout

